{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd83ad4c",
   "metadata": {},
   "source": [
    "# Assignment #2 - Data Gathering and Warehousing - DSSA-5102"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f5c357",
   "metadata": {},
   "source": [
    "Instructor: Melissa Laurino</br>\n",
    "Spring 2024</br></br>\n",
    "Name: Louise Ramos</br>\n",
    "Date: 02/01/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9caf54a",
   "metadata": {},
   "source": [
    "Our next objective is to choose <b>ONE</b> of the datasets from our previous assignment to explore further. The datasets we have chose for Assignment #1 are manageable to clean in R (Or Python if that is what you prefer to explore, see the technology check for working with Python in R in Jupyter notebook). Depending on your data, and especially the size of it, it may be more beneficial to clean in a language we are comfortable working in already instead of cleaning our data in SQL. SQL may be needed for cleaning of databases that are very large or hundreds of terabytes in size. We will clean our datasets first before we attempt to load them into our SQL databases. </br>\n",
    "Not only is data everywhere, but it can also be messy.Â Messy data can originate in the data collection process, whether this is occurring with manual data entry and typos, or with outdated collection forms that hold multiple variables that mean the same thing. For example, while collecting data on marine mammals, it is important to note who the observer is. With Python and R, reading excel or csv files, these languages will take the same variable written as, \"Melissa Laurino\" and \"melissa laurino\" as two separate observers because they are case sensitive. However, this is not accurate because they are meant to be the same person within the observer column or category.</br>\n",
    "Clean data is important for consistency that leads to accurate results and analysis. If we are using our data to make informed decisions in our field, we need it to be clean. We do not want to omit rows that may make a difference to our dataset because they do not fit a certain criteria due to typos, but how much should the original dataset be altered? Depending on your field, there may be regulations and compliance standards regarding data quality. Protocols may state if the data does not read exactly how it should be, then it should be omitted. </br>\n",
    "For our learning objectives in this class, we will clean our data. Our first assignment in our warehousing journey was important because it allowed us to gain a better understanding of a dataset that we personally did not collect. Now that we have that understanding, we can explore it in greater depth and clean it as necessary.<br>\n",
    "<br>\n",
    "It is important when cleaning data to: <br>\n",
    "*Make detailed comments with your code* <br>\n",
    "*Record EVERYTHING omitted and changed if necessary* <br>\n",
    "*Since we are exploring and learning without a specific organization policy, use your best judgement when omitting records. If you have chosen to omit data, please explain why.*</br>\n",
    "<br>\n",
    "<b>The code that I have written below is just to give you ideas on exploring and cleaning data. It is encouraged that you explore and clean it in greater detail than what I have written below for full credit.</b><br>\n",
    "Additional examples: https://epirhandbook.com/en/cleaning-data-and-core-functions.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f29bba7",
   "metadata": {},
   "source": [
    "Dataset name:</b> Provisional COVID 19 Deaths<br>\n",
    "Company/Government Organization:</b> CDC<br>\n",
    "Download link: https://data.cdc.gov/NCHS/Provisional-COVID-19-death-counts-and-rates-by-mon/yrur-wghw/about_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49eb1b9",
   "metadata": {},
   "source": [
    "Load necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "19aaec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce47a72",
   "metadata": {},
   "source": [
    "Load data into Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8ec0dfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_frame = pd.read_csv(\"Provisional_Covid_Deaths.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd089937",
   "metadata": {},
   "source": [
    "Exploration before cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "29f2f5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 555984 data items in the Covid Deaths DataFrame.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the number of elements in the DataFrame\n",
    "frame_elements = covid_frame.size\n",
    "\n",
    "# Printing the number of elements and data types.\n",
    "print(f\"There are {frame_elements} data items in the Covid Deaths DataFrame.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b24bbe81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_as_of                    0\n",
      "jurisdiction_residence        0\n",
      "year                          0\n",
      "month                         0\n",
      "group                         0\n",
      "subgroup1                     0\n",
      "subgroup2                  9504\n",
      "COVID_deaths              10136\n",
      "crude_COVID_rate          13680\n",
      "aa_COVID_rate             39175\n",
      "crude_COVID_rate_ann      13680\n",
      "aa_COVID_rate_ann         39175\n",
      "footnote                  29088\n",
      "dtype: int64 \n",
      "\n",
      "data_as_of                 0.0\n",
      "jurisdiction_residence     0.0\n",
      "year                       0.0\n",
      "month                      0.0\n",
      "group                      0.0\n",
      "subgroup1                  0.0\n",
      "subgroup2                 22.2\n",
      "COVID_deaths              23.7\n",
      "crude_COVID_rate          32.0\n",
      "aa_COVID_rate             91.6\n",
      "crude_COVID_rate_ann      32.0\n",
      "aa_COVID_rate_ann         91.6\n",
      "footnote                  68.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Counting missing values (read is as NaN) across the DataFrame\n",
    "missing_count = covid_frame.isna().sum()\n",
    "\n",
    "# Get frame size\n",
    "frame_size = len(covid_frame)\n",
    "# Put missing data in terms of percent and round to one decimal place.\n",
    "missing_percent = round((missing_count/frame_size) *100, 1)\n",
    "\n",
    "# Printing the count\n",
    "print(f\"{missing_count} \\n\")\n",
    "print(missing_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a45d5b",
   "metadata": {},
   "source": [
    "What columns are missing values (If any)? Do you think you should remove the rows of data at this time in the exploration? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25ef7d5",
   "metadata": {},
   "source": [
    "The main columns missing values are rate columns due to such a small number of reported covid deaths for that subgroup/ pair of subgroups. Also, some rows only contain a single subgroup and so are counted as missing subgroup2. I will not remove all the rows containing NAs in any column but do plan on checking that the missing items in subgroup2 are meant to be missing and not an error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973f5d2b",
   "metadata": {},
   "source": [
    "If you chose to remove rows with specific missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e2cc5579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [data_as_of, jurisdiction_residence, year, month, group, subgroup1, subgroup2, COVID_deaths, crude_COVID_rate, aa_COVID_rate, crude_COVID_rate_ann, aa_COVID_rate_ann, footnote]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Check that rows that have \"And\" in the group column contain information in both subgroups. If not, remove the row as an error.\n",
    "rows_to_remove = covid_frame.loc[(covid_frame[\"group\"].str.contains(\"and\")) & covid_frame[\"subgroup2\"].isna()]\n",
    "print(rows_to_remove)\n",
    "\n",
    "# Removing footnote column since it is just contextual information\n",
    "covid_frame.drop([\"data_as_of\", \"footnote\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6a4531",
   "metadata": {},
   "source": [
    "What about duplicates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e0d65abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [jurisdiction_residence, year, month, group, subgroup1, subgroup2, COVID_deaths, crude_COVID_rate, aa_COVID_rate, crude_COVID_rate_ann, aa_COVID_rate_ann]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [jurisdiction_residence, year, month, group, subgroup1, subgroup2, COVID_deaths, crude_COVID_rate, aa_COVID_rate, crude_COVID_rate_ann, aa_COVID_rate_ann]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#Check for any rows that are completely duplicated\n",
    "duplicate_rows = covid_frame[covid_frame.duplicated()]\n",
    "\n",
    "#Print duplicates:\n",
    "print(duplicate_rows)\n",
    "\n",
    "# There are no rows that are completely duplicated. Checking if any rows in the same location also have the same subgroups and timeframe\n",
    "duplicate_subset = covid_frame[covid_frame.duplicated(subset=[\"jurisdiction_residence\", \"year\", \"month\", \"group\", \"subgroup1\", \"subgroup2\"])]\n",
    "\n",
    "#Print duplicates\n",
    "print(duplicate_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c7342f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No duplicate rows to remove since the subset of columns I chose to check on is all important to the data analyses I want to perform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef92c026",
   "metadata": {},
   "source": [
    "Let's revisit the structure and look at the data types for each column. This will be important for SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "557a57fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jurisdiction_residence     object\n",
      "year                        int64\n",
      "month                       int64\n",
      "group                      object\n",
      "subgroup1                  object\n",
      "subgroup2                  object\n",
      "COVID_deaths              float64\n",
      "crude_COVID_rate          float64\n",
      "aa_COVID_rate             float64\n",
      "crude_COVID_rate_ann      float64\n",
      "aa_COVID_rate_ann         float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Checking type of data in column\n",
    "frame_types = covid_frame.dtypes\n",
    "\n",
    "# Print column data types\n",
    "print(frame_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bf38ed9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jurisdiction_residence            object\n",
      "group                             object\n",
      "subgroup1                         object\n",
      "subgroup2                         object\n",
      "COVID_deaths                     float64\n",
      "crude_COVID_rate                 float64\n",
      "aa_COVID_rate                    float64\n",
      "crude_COVID_rate_ann             float64\n",
      "aa_COVID_rate_ann                float64\n",
      "Date                      datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Combine year and month to a single column and convert to a date\n",
    "covid_frame[\"Observation Date\"] = covid_frame[\"month\"].astype(str) + \"-\" + covid_frame[\"year\"].astype(str)\n",
    "covid_frame['Date'] = pd.to_datetime(covid_frame[\"Observation Date\"].apply(lambda x: datetime.strptime(x, '%m-%Y')))\n",
    "\n",
    "# Drop excess columns\n",
    "covid_frame.drop([\"Observation Date\", \"year\", \"month\"], axis=1, inplace=True)\n",
    "\n",
    "# Change first four columns to all strings and covid statistic columns to float64\n",
    "covid_frame = covid_frame.astype({\"jurisdiction_residence\": str, \"group\": str, \"subgroup1\": str, \"subgroup2\": str, \"COVID_deaths\": \"float64\", \"crude_COVID_rate\": \"float64\",\n",
    "                                  \"aa_COVID_rate_ann\": \"float64\", \"aa_COVID_rate\": \"float64\"})\n",
    "\n",
    "#What are the updated column types?\n",
    "print(covid_frame.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4470acec",
   "metadata": {},
   "source": [
    "Changing text characters in your data. Make all column names lowercase. Lowercase is easier to read in SQL when we get to that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5e5b529f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['jurisdiction_residence', 'group', 'subgroup1', 'subgroup2',\n",
      "       'covid_deaths', 'crude_covid_rate', 'aa_covid_rate',\n",
      "       'crude_covid_rate_ann', 'aa_covid_rate_ann', 'date'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Make all column names lowercase:\n",
    "covid_frame.columns = map(str.lower, covid_frame.columns)\n",
    "\n",
    "# Print column names\n",
    "print(covid_frame.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8c5da8",
   "metadata": {},
   "source": [
    "Assignment #1 asked you to create a graph and check for outliers. Are there any outliers in your columns? How can we check for outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "56e3563d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>covid_deaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>32632.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>289.119515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>49.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>67990.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1807.508403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       covid_deaths\n",
       "count  32632.000000\n",
       "mean     289.119515\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.000000\n",
       "75%       49.000000\n",
       "max    67990.000000\n",
       "std     1807.508403"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting summary statistics for covid_deaths columns as rate columns are a derivative\n",
    "covid_frame.describe()[[\"covid_deaths\"]]\n",
    "\n",
    "# There are multiple months with much higher counts due to the nature of the data. There are many months with zero to very few deaths so this skews the statistics \n",
    "# below. Since this is reported deaths and those do not necessarily follow a normal distribution, I would not suggest removing these extremes unless we are wanting \n",
    "# to have a smooth distribution or trend for a particular output or graph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db2b0b2",
   "metadata": {},
   "source": [
    "<b>To create additional steps for data cleaning in Jupyter notebook: </b><br>\n",
    "Hit the plus button in the top left corner to add a row of code. <br>\n",
    "To change from code to text or headers, select from the drop down menu above. <br>\n",
    "Use \"< b r >\" (No spaces or quotes) to skip a line in markdown and other HTML text font options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4320b4e5",
   "metadata": {},
   "source": [
    "Additional step #1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "81661bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      jurisdiction_residence         group           subgroup1  \\\n",
      "0              United States           Sex              Female   \n",
      "1              United States           Sex                Male   \n",
      "2              United States           Age           0-4 years   \n",
      "3              United States           Age         12-17 years   \n",
      "4              United States           Age         18-29 years   \n",
      "...                      ...           ...                 ...   \n",
      "42763              Region 10  Race and Age  Non-Hispanic White   \n",
      "42764              Region 10  Race and Age  Non-Hispanic White   \n",
      "42765              Region 10  Race and Age  Non-Hispanic White   \n",
      "42766              Region 10  Race and Age  Non-Hispanic White   \n",
      "42767              Region 10  Race and Age  Non-Hispanic White   \n",
      "\n",
      "               subgroup2  covid_deaths  crude_covid_rate  aa_covid_rate  \\\n",
      "0                                  3.0               NaN            NaN   \n",
      "1                                  3.0               NaN            NaN   \n",
      "2                                  0.0               0.0            NaN   \n",
      "3                                  0.0               0.0            NaN   \n",
      "4                                  0.0               0.0            NaN   \n",
      "...                  ...           ...               ...            ...   \n",
      "42763        40-49 years           NaN               NaN            NaN   \n",
      "42764         5-11 years           0.0               0.0            NaN   \n",
      "42765        50-64 years          10.0               NaN            NaN   \n",
      "42766        65-74 years          54.0               4.1            NaN   \n",
      "42767  75 years and over         167.0              20.7            NaN   \n",
      "\n",
      "       crude_covid_rate_ann  aa_covid_rate_ann       date  \n",
      "0                       NaN                NaN 2020-01-01  \n",
      "1                       NaN                NaN 2020-01-01  \n",
      "2                       0.0                NaN 2020-01-01  \n",
      "3                       0.0                NaN 2020-01-01  \n",
      "4                       0.0                NaN 2020-01-01  \n",
      "...                     ...                ...        ...  \n",
      "42763                   NaN                NaN 2023-12-01  \n",
      "42764                   0.0                NaN 2023-12-01  \n",
      "42765                   NaN                NaN 2023-12-01  \n",
      "42766                  49.3                NaN 2023-12-01  \n",
      "42767                 248.8                NaN 2023-12-01  \n",
      "\n",
      "[42768 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# Fill nans in string columns with \"\" rather than NaN\n",
    "covid_frame[\"subgroup2\"] = covid_frame[\"subgroup2\"].replace(\"nan\", '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92b7f13",
   "metadata": {},
   "source": [
    "Lets save our new CLEAN data :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0449d769",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the newly cleaned dataset as a NEW file:\n",
    "#covid_frame.to_csv('Provisional_Covid_Deaths_Cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
